{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68d44ea",
   "metadata": {},
   "source": [
    "# Using OPENAPI - GPT 4.0 mini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c439ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "base_path = Path(\"Documents\")\n",
    "years = [\"2020_inf_profile\", \"2021_inf_profile\", \n",
    "         \"2022_inf_profile\", \"2023_inf_profile\", \"2024_inf_profile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a20e2a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created group.csv with documents organized by year and race\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "def combine_year_texts():\n",
    "    # Create empty list to store year, race and text data\n",
    "    race_texts = []\n",
    "    \n",
    "    # Iterate through each year folder\n",
    "    for year in years:\n",
    "        year_path = base_path / year\n",
    "        \n",
    "        if year_path.exists():\n",
    "            # Get all txt files in the year folder\n",
    "            txt_files = list(year_path.glob('*.txt'))\n",
    "            \n",
    "            # Process each race document separately\n",
    "            for txt_file in txt_files:\n",
    "                try:\n",
    "                    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                        # Extract race name from filename\n",
    "                        race_name = txt_file.stem.split('-')[0].strip()\n",
    "                        \n",
    "                        # Add as separate row with year, race and content\n",
    "                        race_texts.append({\n",
    "                            'year': year.split('-')[0],  # Extract year from folder name\n",
    "                            'race': race_name,\n",
    "                            'text': f.read().strip()  # Remove extra whitespace\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {txt_file}: {e}\")\n",
    "    \n",
    "    # Create DataFrame with columns for year, race and text\n",
    "    df = pd.DataFrame(race_texts)\n",
    "    \n",
    "    # Sort by year and race\n",
    "    df = df.sort_values(['year', 'race'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('goldstandard/group.csv', index=False)\n",
    "    print(\"Created group.csv with documents organized by year and race\")\n",
    "\n",
    "# Run the function\n",
    "combine_year_texts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c1e3440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Multiple model clients initialized\n",
      "\n",
      "Available models:\n",
      "  - gpt-4o-mini\n",
      "  - groq-llama\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize model clients\n",
    "models_config = {\n",
    "    'gpt-4o-mini': {\n",
    "        'provider': 'openai',\n",
    "        'client': OpenAI(api_key=os.getenv('OPENAI_API_KEY')),\n",
    "        'model_name': 'gpt-4o-mini',\n",
    "        'level1_model': 'gpt-4o-mini',\n",
    "        'level2_model': 'gpt-4o',\n",
    "        'level3_model': 'gpt-4o'\n",
    "    },\n",
    "    'groq-llama': {\n",
    "        'provider': 'groq',\n",
    "        'client': OpenAI(\n",
    "            api_key=os.getenv('GROQ_API_KEY'),\n",
    "            base_url=\"https://api.groq.com/openai/v1\"\n",
    "        ),\n",
    "        'model_name': 'llama-3.3-70b-versatile\"',\n",
    "        'level1_model': 'llama-3.3-70b-versatile',\n",
    "        'level2_model': 'llama-3.3-70b-versatile',\n",
    "        'level3_model': 'llama-3.3-70b-versatile'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Multiple model clients initialized\")\n",
    "print(f\"\\nAvailable models:\")\n",
    "for model_key in models_config.keys():\n",
    "    print(f\"  - {model_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76db7e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Universal model calling function created\n"
     ]
    }
   ],
   "source": [
    "def call_model(model_config, system_message, user_message, temperature=0.3, level='level1'):\n",
    "    \"\"\"\n",
    "    Universal function to call different model APIs\n",
    "    \"\"\"\n",
    "    provider = model_config['provider']\n",
    "    \n",
    "    try:\n",
    "        if provider == 'openai' or provider == 'groq':\n",
    "            # OpenAI-compatible API (OpenAI, Groq)\n",
    "            client = model_config['client']\n",
    "            model_name = model_config[f'{level}_model']\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()['message']['content']\n",
    "            else:\n",
    "                raise Exception(f\"Ollama API error: {response.status_code}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error calling {provider} model: {str(e)}\")\n",
    "\n",
    "print(\"✓ Universal model calling function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74dd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main processing function created\n"
     ]
    }
   ],
   "source": [
    "# def process_all_levels(model_key, model_config, df, chunk_size=8):\n",
    "#     \"\"\"\n",
    "#     Process all 3 levels of summarization for a given model\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"PROCESSING MODEL: {model_key}\")\n",
    "#     print(f\"{'='*60}\\n\")\n",
    "    \n",
    "#     chunk_summaries = []\n",
    "#     year_summaries = []\n",
    "#     final_summary = None\n",
    "    \n",
    "#     # Group by year\n",
    "#     grouped_by_year = df.groupby('year')\n",
    "    \n",
    "#     # LEVEL 1: Chunk Summaries\n",
    "#     print(\"Level 1: Generating chunk summaries...\")\n",
    "#     for year, group in tqdm(grouped_by_year, desc=f\"[{model_key}] Years\"):\n",
    "#         races = group['text'].tolist()\n",
    "#         race_chunks = [races[i:i + chunk_size] for i in range(0, len(races), chunk_size)]\n",
    "        \n",
    "#         for i, chunk in enumerate(race_chunks):\n",
    "#             chunk_text = \" \".join(chunk)\n",
    "            \n",
    "#             system_msg = \"You are a data analyst extracting statistics from F1 steward decisions. Provide only factual counts and percentages, no interpretations.\"\n",
    "            \n",
    "#             user_msg = f\"\"\"Analyze these F1 stewards' decisions from {year} (races {i*chunk_size+1}-{min((i+1)*chunk_size, len(races))}) and create a DATA-DRIVEN summary (exactly 100 words):\n",
    "\n",
    "# {chunk_text}\n",
    "\n",
    "# Required format:\n",
    "# \"This chunk covers [#] incidents. Infraction breakdown: [type]: [#] cases ([%]%), [type]: [#] cases ([%]%). Driver involvement: Driver [#] had [#] incidents, Driver [#] had [#]. Penalties: [#] time penalties, [#] fines, [#] reprimands, [#] no action. Sessions: [#] race, [#] qualifying, [#] practice. Common violations: [list top 3 with counts].\"\n",
    "\n",
    "# Include ONLY factual counts and percentages. No interpretations.\"\"\"\n",
    "            \n",
    "#             try:\n",
    "#                 summary = call_model(model_config, system_msg, user_msg, temperature=0.3, level='level1')\n",
    "#                 chunk_summaries.append({\n",
    "#                     'year': year,\n",
    "#                     'chunk': i+1,\n",
    "#                     'summary': summary,\n",
    "#                     'model': model_key\n",
    "#                 })\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  ✗ Error: {year} chunk {i+1}: {e}\")\n",
    "    \n",
    "#     print(f\"  ✓ Level 1 complete: {len(chunk_summaries)} chunks\")\n",
    "    \n",
    "#     # LEVEL 2: Yearly Summaries\n",
    "#     print(\"\\nLevel 2: Generating yearly summaries...\")\n",
    "#     chunk_df = pd.DataFrame(chunk_summaries)\n",
    "#     grouped_chunks = chunk_df.groupby('year')\n",
    "    \n",
    "#     for year, chunks in tqdm(grouped_chunks, desc=f\"[{model_key}] Yearly\"):\n",
    "#         combined_chunks = \" \".join(chunks['summary'].tolist())\n",
    "        \n",
    "#         system_msg = \"You are a statistical analyst creating data-rich narratives from F1 incident data.\"\n",
    "        \n",
    "#         user_msg = f\"\"\"Combine these chunk summaries into a comprehensive {year} statistical summary (exactly 200 words):\n",
    "\n",
    "# {combined_chunks}\n",
    "\n",
    "# Required narrative format with embedded statistics:\n",
    "# \"In {year}, Mercedes accumulated [#] total incidents across [#] races. The infraction distribution showed [type] as the leading category with [#] incidents ([%]%), followed by [type] at [#] incidents ([%]%), and [type] with [#] incidents ([%]%). Driver [#] was involved in [#] incidents ([%]% of yearly total), while Driver [#] accounted for [#] incidents ([%]%). Penalty-wise, [#]% resulted in time penalties, [#]% in fines, [#]% in reprimands, and [#]% received no action. Session analysis revealed [#]% occurred during races, [#]% in qualifying, [#]% in practice. The most frequent violations were [list top 3 with counts]. Monthly distribution peaked in [month] with [#] incidents.\"\n",
    "\n",
    "# Use only statistics from chunk summaries. Write as flowing narrative with embedded numbers. No bullet points or headers.\"\"\"\n",
    "        \n",
    "#         try:\n",
    "#             summary = call_model(model_config, system_msg, user_msg, temperature=0.3, level='level2')\n",
    "#             year_summaries.append({\n",
    "#                 'year': year,\n",
    "#                 'summary': summary,\n",
    "#                 'model': model_key\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             print(f\"  ✗ Error: {year}: {e}\")\n",
    "    \n",
    "#     print(f\"  ✓ Level 2 complete: {len(year_summaries)} years\")\n",
    "    \n",
    "#     # LEVEL 3: Final Summary\n",
    "#     print(\"\\nLevel 3: Generating final summary...\")\n",
    "#     all_year_summaries = \" \".join([ys['summary'] for ys in year_summaries])\n",
    "    \n",
    "#     system_msg = \"You are creating a concise statistical profile across multiple years of F1 data.\"\n",
    "    \n",
    "#     user_msg = f\"\"\"Create a comprehensive 2020-2024 Mercedes infringement profile (200-250 words) as a statistical narrative:\n",
    "\n",
    "# {all_year_summaries}\n",
    "\n",
    "# Write as continuous flowing paragraphs:\n",
    "\n",
    "# \"Over 2020-2024, Mercedes accumulated [total #] FIA infractions. The distribution showed [year] with [#] incidents ([%]%), [year] with [#] ([%]%), through [year] with [#] ([%]%). The leading infraction category was [type] with [#] cases ([%]%), followed by [type] at [%] and [type] at [%]. \n",
    "\n",
    "# Penalty-wise, [%]% resulted in time penalties, [%]% in fines, [%]% in reprimands, and [%]% received no action. \n",
    "\n",
    "# Driver analysis revealed that in 2020 Driver [#] caused [%]% of incidents, in 2021 Driver [#] caused [%]%, continuing through 2024. Overall, Driver [#] accumulated [#] infractions including [#] [type], [#] [type], while Driver [#] had [#] total with [breakdown]. \n",
    "\n",
    "# The peak year was [year] with [#] infractions, averaging [#] per year across the period. Session distribution showed [%]% in races, [%]% in qualifying, [%]% in practice. The trend from 2020 to 2024 showed a [%]% [increase/decrease].\"\n",
    "\n",
    "# CRITICAL: \n",
    "# - Exactly 200-250 words\n",
    "# - Continuous narrative paragraphs, no bullet points\n",
    "# - Embed ALL key statistics naturally\n",
    "# - No strategic insights or interpretations\n",
    "# - Pure data storytelling\"\"\"\n",
    "    \n",
    "#     try:\n",
    "#         final_summary = call_model(model_config, system_msg, user_msg, temperature=0.3, level='level3')\n",
    "#         print(f\"  ✓ Level 3 complete\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "#     return chunk_summaries, year_summaries, final_summary\n",
    "\n",
    "# print(\"✓ Main processing function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb43ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main processing function updated with accurate counting\n"
     ]
    }
   ],
   "source": [
    "# def process_all_levels(model_key, model_config, df, chunk_size=8):\n",
    "#     \"\"\"\n",
    "#     Process all 3 levels of summarization for a given model\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"PROCESSING MODEL: {model_key}\")\n",
    "#     print(f\"{'='*60}\\n\")\n",
    "    \n",
    "#     chunk_summaries = []\n",
    "#     year_summaries = []\n",
    "#     final_summary = None\n",
    "    \n",
    "#     # Group by year\n",
    "#     grouped_by_year = df.groupby('year')\n",
    "    \n",
    "#     # LEVEL 1: Chunk Summaries\n",
    "#     print(\"Level 1: Generating chunk summaries...\")\n",
    "#     for year, group in tqdm(grouped_by_year, desc=f\"[{model_key}] Years\"):\n",
    "#         num_rows = len(group)\n",
    "        \n",
    "#         # Chunk the DATAFRAME, not just text\n",
    "#         for i in range(0, num_rows, chunk_size):\n",
    "#             chunk_df = group.iloc[i:i + chunk_size]\n",
    "            \n",
    "#             # ACTUAL count from dataframe rows\n",
    "#             actual_count = len(chunk_df)\n",
    "            \n",
    "#             # Combine text for LLM context\n",
    "#             chunk_text = \" \".join(chunk_df['text'].tolist())\n",
    "            \n",
    "#             system_msg = \"You are a data analyst summarizing F1 steward decisions. Use the provided statistics.\"\n",
    "            \n",
    "#             user_msg = f\"\"\"Summarize these {year} F1 steward decisions (exactly 100 words):\n",
    "\n",
    "# VERIFIED COUNT: {actual_count} infringement decisions in this chunk (DO NOT COUNT YOURSELF)\n",
    "\n",
    "# TEXT FOR ANALYSIS:\n",
    "# {chunk_text}\n",
    "\n",
    "# Write a summary following this structure:\n",
    "# \"This chunk contains {actual_count} infringement decisions. [Analyze the text to describe]: Infraction type breakdown with approximate counts. Drivers involved with frequencies. Penalty types distribution. Session breakdown. Common violation patterns.\"\n",
    "\n",
    "# CRITICAL: Use the exact number {actual_count} for total infractions. Extract other details by analyzing the text content.\"\"\"\n",
    "            \n",
    "#             try:\n",
    "#                 summary = call_model(model_config, system_msg, user_msg, temperature=0.2, level='level1')\n",
    "#                 chunk_summaries.append({\n",
    "#                     'year': year,\n",
    "#                     'chunk': (i // chunk_size) + 1,\n",
    "#                     'summary': summary,\n",
    "#                     'model': model_key,\n",
    "#                     'verified_count': actual_count  # Store for validation\n",
    "#                 })\n",
    "#                 print(f\"  ✓ {year} chunk {(i // chunk_size) + 1}: {actual_count} infractions\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  ✗ Error: {year} chunk {(i // chunk_size) + 1}: {e}\")\n",
    "    \n",
    "#     print(f\"  ✓ Level 1 complete: {len(chunk_summaries)} chunks\")\n",
    "    \n",
    "#     # Validate counts\n",
    "#     chunk_df = pd.DataFrame(chunk_summaries)\n",
    "#     if 'verified_count' in chunk_df.columns:\n",
    "#         print(f\"\\n  Validation:\")\n",
    "#         for year in chunk_df['year'].unique():\n",
    "#             year_chunks = chunk_df[chunk_df['year'] == year]\n",
    "#             total_from_chunks = year_chunks['verified_count'].sum()\n",
    "#             actual_from_data = len(df[df['year'] == year])\n",
    "#             match = \"✓\" if total_from_chunks == actual_from_data else \"✗\"\n",
    "#             print(f\"    {year}: {total_from_chunks} (chunks) vs {actual_from_data} (data) {match}\")\n",
    "    \n",
    "#     # LEVEL 2: Yearly Summaries\n",
    "#     print(\"\\nLevel 2: Generating yearly summaries...\")\n",
    "#     chunk_df = pd.DataFrame(chunk_summaries)\n",
    "#     grouped_chunks = chunk_df.groupby('year')\n",
    "    \n",
    "#     for year, chunks in tqdm(grouped_chunks, desc=f\"[{model_key}] Yearly\"):\n",
    "#         # Get total verified count for this year\n",
    "#         year_total = chunks['verified_count'].sum() if 'verified_count' in chunks.columns else 'unknown'\n",
    "        \n",
    "#         combined_chunks = \" \".join(chunks['summary'].tolist())\n",
    "        \n",
    "#         system_msg = \"You are a statistical analyst creating data-rich narratives from F1 incident data.\"\n",
    "        \n",
    "#         user_msg = f\"\"\"Combine these chunk summaries into a comprehensive {year} statistical summary (exactly 200 words):\n",
    "\n",
    "# VERIFIED TOTAL FOR {year}: {year_total} total infringement decisions\n",
    "\n",
    "# CHUNK SUMMARIES:\n",
    "# {combined_chunks}\n",
    "\n",
    "# Required narrative format with embedded statistics:\n",
    "# \"In {year}, Mercedes accumulated {year_total} total infringement decisions. The infraction distribution showed [type] as the leading category with [#] incidents ([%]%), followed by [type] at [#] incidents ([%]%), and [type] with [#] incidents ([%]%). Driver [#] was involved in [#] incidents ([%]% of yearly total), while Driver [#] accounted for [#] incidents ([%]%). Penalty-wise, [#]% resulted in time penalties, [#]% in fines, [#]% in reprimands, and [#]% received no action. Session analysis revealed [#]% occurred during races, [#]% in qualifying, [#]% in practice. The most frequent violations were [list top 3 with counts].\"\n",
    "\n",
    "# Use EXACT total {year_total}. Extract other statistics from chunk summaries. Write as flowing narrative with embedded numbers. No bullet points.\"\"\"\n",
    "        \n",
    "#         try:\n",
    "#             summary = call_model(model_config, system_msg, user_msg, temperature=0.2, level='level2')\n",
    "#             year_summaries.append({\n",
    "#                 'year': year,\n",
    "#                 'summary': summary,\n",
    "#                 'model': model_key,\n",
    "#                 'verified_count': year_total\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             print(f\"  ✗ Error: {year}: {e}\")\n",
    "    \n",
    "#     print(f\"  ✓ Level 2 complete: {len(year_summaries)} years\")\n",
    "    \n",
    "#     # LEVEL 3: Final Summary\n",
    "#     print(\"\\nLevel 3: Generating final summary...\")\n",
    "    \n",
    "#     # Calculate grand total\n",
    "#     grand_total = sum([ys['verified_count'] for ys in year_summaries if isinstance(ys['verified_count'], (int, float))])\n",
    "    \n",
    "#     all_year_summaries = \" \".join([ys['summary'] for ys in year_summaries])\n",
    "    \n",
    "#     system_msg = \"You are creating a concise statistical profile across multiple years of F1 data.\"\n",
    "    \n",
    "#     user_msg = f\"\"\"Create a comprehensive 2020-2024 Mercedes infringement profile (200-250 words) as a statistical narrative:\n",
    "\n",
    "# VERIFIED GRAND TOTAL: {grand_total} total infringement decisions across 2020-2024\n",
    "\n",
    "# YEARLY SUMMARIES:\n",
    "# {all_year_summaries}\n",
    "\n",
    "# Write as continuous flowing paragraphs:\n",
    "\n",
    "# \"Over 2020-2024, Mercedes accumulated {grand_total} FIA infringement decisions. The distribution showed [year] with [#] incidents ([%]%), [year] with [#] ([%]%), through [year] with [#] ([%]%). The leading infraction category was [type] with [#] cases ([%]%), followed by [type] at [%] and [type] at [%]. \n",
    "\n",
    "# Penalty-wise, [%]% resulted in time penalties, [%]% in fines, [%]% in reprimands, and [%]% received no action. \n",
    "\n",
    "# Driver analysis revealed that in 2020 Driver [#] caused [%]% of incidents, in 2021 Driver [#] caused [%]%, continuing through 2024. Overall, Driver [#] accumulated [#] infractions including [#] [type], [#] [type], while Driver [#] had [#] total with [breakdown]. \n",
    "\n",
    "# The peak year was [year] with [#] infractions, averaging [#] per year across the period. Session distribution showed [%]% in races, [%]% in qualifying, [#]% in practice. The trend from 2020 to 2024 showed a [%]% [increase/decrease].\"\n",
    "\n",
    "# CRITICAL: \n",
    "# - Use EXACT total {grand_total}\n",
    "# - Exactly 200-250 words\n",
    "# - Continuous narrative paragraphs, no bullet points\n",
    "# - Embed statistics naturally\n",
    "# - Pure data storytelling\"\"\"\n",
    "    \n",
    "#     try:\n",
    "#         final_summary = call_model(model_config, system_msg, user_msg, temperature=0.2, level='level3')\n",
    "#         print(f\"  ✓ Level 3 complete\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "#     return chunk_summaries, year_summaries, final_summary\n",
    "\n",
    "# print(\"✓ Main processing function updated with accurate counting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64f483b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main processing function updated with accurate counting and validation\n"
     ]
    }
   ],
   "source": [
    "def process_all_levels(model_key, model_config, df, chunk_size=8):\n",
    "    \"\"\"\n",
    "    Process all 3 levels of summarization for a given model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING MODEL: {model_key}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    chunk_summaries = []\n",
    "    year_summaries = []\n",
    "    final_summary = None\n",
    "    \n",
    "    # Group by year\n",
    "    grouped_by_year = df.groupby('year')\n",
    "    \n",
    "    # LEVEL 1: Chunk Summaries\n",
    "    print(\"Level 1: Generating chunk summaries...\")\n",
    "    for year, group in tqdm(grouped_by_year, desc=f\"[{model_key}] Years\"):\n",
    "        num_rows = len(group)\n",
    "        \n",
    "        # Chunk the DATAFRAME, not just text\n",
    "        for i in range(0, num_rows, chunk_size):\n",
    "            chunk_df = group.iloc[i:i + chunk_size]\n",
    "            \n",
    "            # ACTUAL count from dataframe rows\n",
    "            actual_count = len(chunk_df)\n",
    "            \n",
    "            # Combine text for LLM context\n",
    "            chunk_text = \" \".join(chunk_df['text'].tolist())\n",
    "            \n",
    "            system_msg = \"You are a data analyst summarizing F1 steward decisions. Use the provided statistics.\"\n",
    "            \n",
    "            user_msg = f\"\"\"Summarize these {year} F1 steward decisions (exactly 100 words):\n",
    "\n",
    "VERIFIED COUNT: {actual_count} infringement decisions in this chunk (DO NOT COUNT YOURSELF)\n",
    "\n",
    "TEXT FOR ANALYSIS:\n",
    "{chunk_text}\n",
    "\n",
    "Write a summary following this structure:\n",
    "\"This chunk contains {actual_count} infringement decisions. [Analyze the text to describe]: Infraction type breakdown with approximate counts. Drivers involved with frequencies. Penalty types distribution. Session breakdown. Common violation patterns.\"\n",
    "\n",
    "CRITICAL: Use the exact number {actual_count} for total infractions. Extract other details by analyzing the text content.\"\"\"\n",
    "            \n",
    "            try:\n",
    "                summary = call_model(model_config, system_msg, user_msg, temperature=0.2, level='level1')\n",
    "                chunk_summaries.append({\n",
    "                    'year': year,\n",
    "                    'chunk': (i // chunk_size) + 1,\n",
    "                    'summary': summary,\n",
    "                    'model': model_key,\n",
    "                    'verified_count': actual_count  # Store for validation\n",
    "                })\n",
    "                print(f\"  ✓ {year} chunk {(i // chunk_size) + 1}: {actual_count} infractions\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {year} chunk {(i // chunk_size) + 1}: {e}\")\n",
    "    \n",
    "    print(f\"  ✓ Level 1 complete: {len(chunk_summaries)} chunks\")\n",
    "    \n",
    "    # Validate counts\n",
    "    chunk_df_validation = pd.DataFrame(chunk_summaries)\n",
    "    if 'verified_count' in chunk_df_validation.columns:\n",
    "        print(f\"\\n  Validation:\")\n",
    "        for year in chunk_df_validation['year'].unique():\n",
    "            year_chunks = chunk_df_validation[chunk_df_validation['year'] == year]\n",
    "            total_from_chunks = year_chunks['verified_count'].sum()\n",
    "            actual_from_data = len(df[df['year'] == year])\n",
    "            match = \"✓\" if total_from_chunks == actual_from_data else \"✗\"\n",
    "            print(f\"    {year}: {total_from_chunks} (chunks) vs {actual_from_data} (data) {match}\")\n",
    "    \n",
    "    # LEVEL 2: Yearly Summaries\n",
    "    print(\"\\nLevel 2: Generating yearly summaries...\")\n",
    "    chunk_df = pd.DataFrame(chunk_summaries)\n",
    "    grouped_chunks = chunk_df.groupby('year')\n",
    "    \n",
    "    for year, chunks in tqdm(grouped_chunks, desc=f\"[{model_key}] Yearly\"):\n",
    "        # Get total verified count for this year\n",
    "        year_total = int(chunks['verified_count'].sum()) if 'verified_count' in chunks.columns else 0\n",
    "        \n",
    "        combined_chunks = \" \".join(chunks['summary'].tolist())\n",
    "        \n",
    "        system_msg = \"You are a statistical analyst creating data-rich narratives from F1 incident data.\"\n",
    "        \n",
    "        user_msg = f\"\"\"Combine these chunk summaries into a comprehensive {year} statistical summary (exactly 200 words):\n",
    "\n",
    "VERIFIED TOTAL FOR {year}: {year_total} total infringement decisions\n",
    "\n",
    "CHUNK SUMMARIES:\n",
    "{combined_chunks}\n",
    "\n",
    "Required narrative format with embedded statistics:\n",
    "\"In {year}, Mercedes accumulated {year_total} total infringement decisions. The infraction distribution showed [type] as the leading category with [#] incidents ([%]%), followed by [type] at [#] incidents ([%]%), and [type] with [#] incidents ([%]%). Driver [#] was involved in [#] incidents ([%]% of yearly total), while Driver [#] accounted for [#] incidents ([%]%). Penalty-wise, [#]% resulted in time penalties, [#]% in fines, [#]% in reprimands, and [#]% received no action. Session analysis revealed [#]% occurred during races, [#]% in qualifying, [#]% in practice. The most frequent violations were [list top 3 with counts].\"\n",
    "\n",
    "Use EXACT total {year_total}. Extract other statistics from chunk summaries. Write as flowing narrative with embedded numbers. No bullet points.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            summary = call_model(model_config, system_msg, user_msg, temperature=0.2, level='level2')\n",
    "            year_summaries.append({\n",
    "                'year': year,\n",
    "                'summary': summary,\n",
    "                'model': model_key,\n",
    "                'verified_count': year_total\n",
    "            })\n",
    "            print(f\"  ✓ {year}: {year_total} infractions\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {year}: {e}\")\n",
    "    \n",
    "    print(f\"  ✓ Level 2 complete: {len(year_summaries)} years\")\n",
    "    \n",
    "    # LEVEL 3: Final Summary\n",
    "    print(\"\\nLevel 3: Generating final summary...\")\n",
    "    \n",
    "    # Calculate grand total from ORIGINAL DATAFRAME (most reliable)\n",
    "    grand_total = len(df)\n",
    "    \n",
    "    # Alternative: sum from year_summaries (should match)\n",
    "    grand_total_from_years = sum([ys['verified_count'] for ys in year_summaries if isinstance(ys['verified_count'], (int, float))])\n",
    "    \n",
    "    # Verify they match\n",
    "    if grand_total != grand_total_from_years:\n",
    "        print(f\"  ⚠ WARNING: Mismatch! DataFrame: {grand_total}, Year summaries: {grand_total_from_years}\")\n",
    "        print(f\"  Using DataFrame count: {grand_total}\")\n",
    "    \n",
    "    all_year_summaries = \" \".join([ys['summary'] for ys in year_summaries])\n",
    "    \n",
    "    system_msg = \"You are creating a concise statistical profile across multiple years of F1 data.\"\n",
    "    \n",
    "    user_msg = f\"\"\"Create a comprehensive 2020-2024 Mercedes infringement profile (200-250 words) as a statistical narrative:\n",
    "\n",
    "VERIFIED GRAND TOTAL: {grand_total} total infringement decisions across 2020-2024\n",
    "\n",
    "DO NOT COUNT OR ADD UP NUMBERS YOURSELF. THE TOTAL IS EXACTLY {grand_total}.\n",
    "\n",
    "YEARLY SUMMARIES:\n",
    "{all_year_summaries}\n",
    "\n",
    "Your response MUST begin with this EXACT sentence:\n",
    "\"Over 2020-2024, Mercedes accumulated {grand_total} FIA infringement decisions.\"\n",
    "\n",
    "Then continue with flowing paragraphs covering:\n",
    "- Year-by-year distribution with counts and percentages\n",
    "- Leading infraction categories with counts and percentages\n",
    "- Penalty type breakdown with percentages\n",
    "- Driver-specific patterns across years\n",
    "- Peak year identification and yearly average\n",
    "- Session distribution percentages\n",
    "- Overall trend from 2020 to 2024\n",
    "\n",
    "CRITICAL RULES:\n",
    "- START with: \"Over 2020-2024, Mercedes accumulated {grand_total} FIA infringement decisions.\"\n",
    "- Use ONLY {grand_total} as the total - do not calculate or add years yourself\n",
    "- Extract all other statistics from yearly summaries\n",
    "- Write as continuous flowing narrative, NO bullet points or headers\n",
    "- Exactly 200-250 words\n",
    "- Pure data storytelling with embedded statistics\"\"\"\n",
    "    \n",
    "    try:\n",
    "        final_summary = call_model(model_config, system_msg, user_msg, temperature=0.1, level='level3')\n",
    "        \n",
    "        # Validate the summary contains correct total\n",
    "        if str(grand_total) not in final_summary:\n",
    "            print(f\"  ⚠ WARNING: Final summary doesn't mention {grand_total}!\")\n",
    "            print(f\"  Summary preview: {final_summary[:200]}...\")\n",
    "        \n",
    "        print(f\"  ✓ Level 3 complete (Grand Total: {grand_total})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    return chunk_summaries, year_summaries, final_summary\n",
    "\n",
    "print(\"✓ Main processing function updated with accurate counting and validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c75de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded: 82 rows\n",
      "  Years: ['2020_inf_profile' '2021_inf_profile' '2022_inf_profile'\n",
      " '2023_inf_profile' '2024_inf_profile']\n",
      "  Rows per year:\n",
      "year\n",
      "2020_inf_profile    11\n",
      "2021_inf_profile    16\n",
      "2022_inf_profile    15\n",
      "2023_inf_profile    23\n",
      "2024_inf_profile    17\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('goldstandard/group.csv')\n",
    "\n",
    "print(f\"✓ Data loaded: {len(df)} rows\")\n",
    "print(f\"  Years: {df['year'].unique()}\")\n",
    "print(f\"  Rows per year:\\n{df['year'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d17996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING MODEL: groq-llama\n",
      "============================================================\n",
      "\n",
      "Level 1: Generating chunk summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Years:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2020_inf_profile chunk 1: 8 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Years:  20%|██        | 1/5 [00:02<00:09,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2020_inf_profile chunk 2: 3 infractions\n",
      "  ✓ 2021_inf_profile chunk 1: 8 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Years:  40%|████      | 2/5 [00:04<00:07,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2021_inf_profile chunk 2: 8 infractions\n",
      "  ✓ 2022_inf_profile chunk 1: 8 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Years:  60%|██████    | 3/5 [00:32<00:28, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2022_inf_profile chunk 2: 7 infractions\n",
      "  ✓ 2023_inf_profile chunk 1: 8 infractions\n",
      "  ✓ 2023_inf_profile chunk 2: 8 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Years:  80%|████████  | 4/5 [01:19<00:27, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2023_inf_profile chunk 3: 7 infractions\n",
      "  ✓ 2024_inf_profile chunk 1: 8 infractions\n",
      "  ✓ 2024_inf_profile chunk 2: 8 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Years: 100%|██████████| 5/5 [02:00<00:00, 24.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2024_inf_profile chunk 3: 1 infractions\n",
      "  ✓ Level 1 complete: 12 chunks\n",
      "\n",
      "  Validation:\n",
      "    2020_inf_profile: 11 (chunks) vs 11 (data) ✓\n",
      "    2021_inf_profile: 16 (chunks) vs 16 (data) ✓\n",
      "    2022_inf_profile: 15 (chunks) vs 15 (data) ✓\n",
      "    2023_inf_profile: 23 (chunks) vs 23 (data) ✓\n",
      "    2024_inf_profile: 17 (chunks) vs 17 (data) ✓\n",
      "\n",
      "Level 2: Generating yearly summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Yearly:  20%|██        | 1/5 [00:03<00:14,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2020_inf_profile: 11 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Yearly:  40%|████      | 2/5 [00:06<00:10,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2021_inf_profile: 16 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Yearly:  60%|██████    | 3/5 [00:11<00:07,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2022_inf_profile: 15 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Yearly:  80%|████████  | 4/5 [00:15<00:03,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2023_inf_profile: 23 infractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[groq-llama] Yearly: 100%|██████████| 5/5 [00:20<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 2024_inf_profile: 17 infractions\n",
      "  ✓ Level 2 complete: 5 years\n",
      "\n",
      "Level 3: Generating final summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Level 3 complete (Grand Total: 82)\n",
      "\n",
      "✓ groq-llama processing complete!\n",
      "\n",
      "\n",
      "============================================================\n",
      "ALL MODELS PROCESSED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Store results for all models\n",
    "all_results = {}\n",
    "\n",
    "# Select which models to run (comment out models you don't want to use)\n",
    "models_to_run = [\n",
    "    'gpt-4o-mini',      # OpenAI\n",
    "    'groq-llama',       # Groq with Llama\n",
    "    # 'groq-mixtral',   # Groq with Mixtral\n",
    "    #'ollama-llama',   # Local Ollama (make sure Ollama is running!)\n",
    "]\n",
    "\n",
    "for model_key in models_to_run:\n",
    "    if model_key not in models_config:\n",
    "        print(f\"⚠ Model '{model_key}' not found in config, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        chunk_sums, year_sums, final_sum = process_all_levels(\n",
    "            model_key, \n",
    "            models_config[model_key], \n",
    "            df, \n",
    "            chunk_size=8\n",
    "        )\n",
    "        \n",
    "        all_results[model_key] = {\n",
    "            'chunks': chunk_sums,\n",
    "            'years': year_sums,\n",
    "            'final': final_sum\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n✓ {model_key} processing complete!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error processing {model_key}: {e}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS PROCESSED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22c38c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved results for groq-llama\n",
      "\n",
      "============================================================\n",
      "FILES CREATED:\n",
      "============================================================\n",
      "\n",
      "groq-llama:\n",
      "  - goldstandard/chunk_summaries_groq-llama.csv\n",
      "  - goldstandard/year_summaries_groq-llama.csv\n",
      "  - goldstandard/final_summary_groq-llama.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results for each model\n",
    "for model_key, results in all_results.items():\n",
    "    # Save chunk summaries\n",
    "    chunk_df = pd.DataFrame(results['chunks'])\n",
    "    chunk_df.to_csv(f'goldstandard/chunk_summaries_{model_key}.csv', index=False)\n",
    "    \n",
    "    # Save yearly summaries\n",
    "    year_df = pd.DataFrame(results['years'])\n",
    "    year_df.to_csv(f'goldstandard/year_summaries_{model_key}.csv', index=False)\n",
    "    \n",
    "    # Save final summary\n",
    "    if results['final']:\n",
    "        final_df = pd.DataFrame([{\n",
    "            'summary': results['final'],\n",
    "            'model': model_key\n",
    "        }])\n",
    "        final_df.to_csv(f'goldstandard/final_summary_{model_key}.csv', index=False)\n",
    "    \n",
    "    print(f\"✓ Saved results for {model_key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILES CREATED:\")\n",
    "print(\"=\"*60)\n",
    "for model_key in all_results.keys():\n",
    "    print(f\"\\n{model_key}:\")\n",
    "    print(f\"  - goldstandard/chunk_summaries_{model_key}.csv\")\n",
    "    print(f\"  - goldstandard/year_summaries_{model_key}.csv\")\n",
    "    print(f\"  - goldstandard/final_summary_{model_key}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31718c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>num_chunks</th>\n",
       "      <th>num_years</th>\n",
       "      <th>final_word_count</th>\n",
       "      <th>word_count_ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>groq-llama</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>205</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  num_chunks  num_years  final_word_count  word_count_ok\n",
       "0  groq-llama          12          5               205           True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL SUMMARIES BY MODEL\n",
      "============================================================\n",
      "\n",
      "--- GROQ-LLAMA ---\n",
      "Over 2020-2024, Mercedes accumulated 82 FIA infringement decisions. The yearly distribution shows a varying trend, with 11 incidents in 2020, 16 in 2021, 15 in 2022, 23 in 2023, and 17 in 2024. The leading infraction categories across these years include track limits breaches, technical breaches, and procedural mistakes, with track limits breaches being the most frequent in 2020 and 2023, and technical breaches leading in 2021 and 2024. Penalty-wise, time penalties were the most common, ranging ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for model_key, results in all_results.items():\n",
    "    word_count = len(results['final'].split()) if results['final'] else 0\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'model': model_key,\n",
    "        'num_chunks': len(results['chunks']),\n",
    "        'num_years': len(results['years']),\n",
    "        'final_word_count': word_count,\n",
    "        'word_count_ok': 200 <= word_count <= 250\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "display(comparison_df)\n",
    "\n",
    "# Display final summaries side by side\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARIES BY MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_key, results in all_results.items():\n",
    "    print(f\"\\n--- {model_key.upper()} ---\")\n",
    "    print(results['final'][:500] + \"...\" if len(results['final']) > 500 else results['final'])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc697e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb2d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
