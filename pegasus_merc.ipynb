{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mercedes F1 Infringement Profile - Pegasus Summarization\n",
        "\n",
        "This notebook implements Pegasus (abstractive summarization) for Mercedes F1 infringement documents.\n",
        "\n",
        "## Objective:\n",
        "- Process all `no_footer_` files from the preprocessed dataset\n",
        "- Apply Pegasus model for abstractive summarization\n",
        "- Maintain temporal analysis (year-by-year summaries)\n",
        "- Generate abstractive summaries that create new sentences (not just extract)\n",
        "\n",
        "## Input:\n",
        "- `pre_proc_op/` folder containing `no_footer_*.txt` files organized by year\n",
        "\n",
        "## Output:\n",
        "- Console summaries for each year\n",
        "- Overall consolidated summary\n",
        "- Results saved in `pegasus_results/` folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Pegasus implementation\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "Processed base path: pre_proc_op\n",
            "Years to analyze: ['2020', '2021', '2022', '2023', '2024']\n",
            "Model: google/pegasus-xsum\n",
            "Max summary length: 512\n",
            "Min summary length: 50\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "processed_base_path = Path(\"pre_proc_op\")\n",
        "years = [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]\n",
        "\n",
        "# Pegasus model configuration\n",
        "MODEL_NAME = \"google/pegasus-xsum\"  # XSum variant for abstractive summarization\n",
        "MAX_LENGTH = 512  # Maximum length for generated summary\n",
        "MIN_LENGTH = 50   # Minimum length for generated summary\n",
        "MAX_INPUT_LENGTH = 1024  # Pegasus max input tokens (will chunk if needed)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"Processed base path: {processed_base_path}\")\n",
        "print(f\"Years to analyze: {years}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Max summary length: {MAX_LENGTH}\")\n",
        "print(f\"Min summary length: {MIN_LENGTH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Pegasus model...\n",
            "This may take a few minutes on first run (downloading model)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38b634f08aa94367af8195a2366a76ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec3cf233fe94f2ead17ac85d8fe8673",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa5525db724c4bb98b6c8366d00b031f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Initialize Pegasus summarizer\n",
        "print(\"Loading Pegasus model...\")\n",
        "print(\"This may take a few minutes on first run (downloading model)...\")\n",
        "\n",
        "try:\n",
        "    # Use pipeline for simplicity\n",
        "    summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=MODEL_NAME,\n",
        "        device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
        "        framework=\"pt\"\n",
        "    )\n",
        "    print(f\"Pegasus model loaded successfully\")\n",
        "    print(f\"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Falling back to CPU...\")\n",
        "    summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=MODEL_NAME,\n",
        "        device=-1,\n",
        "        framework=\"pt\"\n",
        "    )\n",
        "    print(\"Pegasus model loaded on CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to chunk long texts\n",
        "def chunk_text(text, max_chars=4000):\n",
        "    \"\"\"\n",
        "    Split text into chunks that fit within token limits.\n",
        "    Pegasus has max_position_embeddings of 1024, so we chunk conservatively.\n",
        "    \"\"\"\n",
        "    if len(text) <= max_chars:\n",
        "        return [text]\n",
        "    \n",
        "    # Split by sentences (rough approximation)\n",
        "    sentences = text.split('. ')\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 2 <= max_chars:\n",
        "            current_chunk += sentence + \". \"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "    \n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Helper function to summarize long texts\n",
        "def summarize_long_text(text, summarizer, max_length=MAX_LENGTH, min_length=MIN_LENGTH):\n",
        "    \"\"\"\n",
        "    Summarize text, handling long inputs by chunking.\n",
        "    \"\"\"\n",
        "    chunks = chunk_text(text)\n",
        "    \n",
        "    if len(chunks) == 1:\n",
        "        # Single chunk - summarize directly\n",
        "        try:\n",
        "            result = summarizer(\n",
        "                text,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=False\n",
        "            )\n",
        "            return result[0]['summary_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Error in summarization: {e}\")\n",
        "            return \"\"\n",
        "    else:\n",
        "        # Multiple chunks - summarize each and combine\n",
        "        chunk_summaries = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            try:\n",
        "                result = summarizer(\n",
        "                    chunk,\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    do_sample=False\n",
        "                )\n",
        "                chunk_summaries.append(result[0]['summary_text'])\n",
        "            except Exception as e:\n",
        "                print(f\"Error summarizing chunk {i+1}/{len(chunks)}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Combine chunk summaries\n",
        "        combined = \" \".join(chunk_summaries)\n",
        "        \n",
        "        # If combined is still too long, summarize again\n",
        "        if len(combined) > 4000:\n",
        "            try:\n",
        "                result = summarizer(\n",
        "                    combined,\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    do_sample=False\n",
        "                )\n",
        "                return result[0]['summary_text']\n",
        "            except Exception as e:\n",
        "                print(f\"Error in final summarization: {e}\")\n",
        "                return combined\n",
        "        \n",
        "        return combined\n",
        "\n",
        "print(\"Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process documents year by year\n",
        "print(\"MERCEDES F1 INFRINGEMENT PROFILE - PEGASUS SUMMARIZATION\")\n",
        "\n",
        "yearly_summaries = {}\n",
        "yearly_stats = {}\n",
        "\n",
        "for year in years:\n",
        "    print(f\"\\nPROCESSING {year} - MERCEDES F1 INFRINGEMENTS\")\n",
        "    \n",
        "    year_path = processed_base_path / year\n",
        "    \n",
        "    if not year_path.exists():\n",
        "        print(f\"Folder {year_path} does not exist\")\n",
        "        continue\n",
        "    \n",
        "    # Get all no_footer_ files\n",
        "    no_footer_files = list(year_path.glob(\"no_footer_*.txt\"))\n",
        "    \n",
        "    if not no_footer_files:\n",
        "        print(f\"No no_footer_ files found in {year}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"Found {len(no_footer_files)} processed documents in {year}\")\n",
        "    \n",
        "    # Read and combine all documents for the year\n",
        "    combined_text = \"\"\n",
        "    total_chars = 0\n",
        "    processed_files = 0\n",
        "    \n",
        "    for file_path in no_footer_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read().strip()\n",
        "                if content:\n",
        "                    combined_text += content + \" \"\n",
        "                    total_chars += len(content)\n",
        "                    processed_files += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path.name}: {e}\")\n",
        "    \n",
        "    if not combined_text.strip():\n",
        "        print(f\"No valid content found in {year}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"Processed {processed_files} files, {total_chars:,} total characters\")\n",
        "    \n",
        "    # Generate summary using Pegasus\n",
        "    print(f\"\\nGenerating Pegasus summary for {year}...\")\n",
        "    print(\"This may take a moment...\")\n",
        "    \n",
        "    summary = summarize_long_text(\n",
        "        combined_text.strip(),\n",
        "        summarizer,\n",
        "        max_length=MAX_LENGTH,\n",
        "        min_length=MIN_LENGTH\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    yearly_summaries[year] = summary\n",
        "    yearly_stats[year] = {\n",
        "        'files_processed': processed_files,\n",
        "        'total_chars': total_chars,\n",
        "        'summary_length': len(summary)\n",
        "    }\n",
        "    \n",
        "    # Display summary\n",
        "    print(f\"\\nPEGASUS SUMMARY FOR {year}:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(summary)\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Summary length: {len(summary):,} characters\")\n",
        "    print(f\"Compression ratio: {(len(summary)/total_chars)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nProcessed {len(yearly_summaries)} years successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate overall consolidated summary\n",
        "print(\"\\nOVERALL CONSOLIDATED SUMMARY - ALL YEARS\")\n",
        "\n",
        "if yearly_summaries:\n",
        "    # Combine all yearly summaries\n",
        "    all_summaries_text = \" \"\n",
        "    for year, summary in yearly_summaries.items():\n",
        "        all_summaries_text += f\"{year} Summary: {summary} \"\n",
        "    \n",
        "    # Generate overall summary\n",
        "    print(\"Generating overall consolidated summary...\")\n",
        "    print(\"This may take a moment...\")\n",
        "    \n",
        "    overall_summary = summarize_long_text(\n",
        "        all_summaries_text.strip(),\n",
        "        summarizer,\n",
        "        max_length=MAX_LENGTH,\n",
        "        min_length=MIN_LENGTH\n",
        "    )\n",
        "    \n",
        "    print(\"\\nOVERALL MERCEDES F1 INFRINGEMENT PROFILE (PEGASUS):\")\n",
        "    print(\"=\" * 50)\n",
        "    print(overall_summary)\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Statistics\n",
        "    total_files = sum(stats['files_processed'] for stats in yearly_stats.values())\n",
        "    total_chars = sum(stats['total_chars'] for stats in yearly_stats.values())\n",
        "    \n",
        "    print(f\"\\nOVERALL STATISTICS:\")\n",
        "    print(f\"Total documents processed: {total_files}\")\n",
        "    print(f\"Total characters analyzed: {total_chars:,}\")\n",
        "    print(f\"Overall summary length: {len(overall_summary):,} characters\")\n",
        "    print(f\"Overall compression ratio: {(len(overall_summary)/total_chars)*100:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nYEARLY BREAKDOWN:\")\n",
        "    for year, stats in yearly_stats.items():\n",
        "        compression = (stats['summary_length']/stats['total_chars'])*100 if stats['total_chars'] > 0 else 0\n",
        "        print(f\"{year}: {stats['files_processed']} docs, {stats['total_chars']:,} chars -> {stats['summary_length']:,} chars ({compression:.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"No summaries generated. Please check the input files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to files\n",
        "print(\"\\nSAVING RESULTS\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(\"pegasus_results\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save yearly summaries\n",
        "for year, summary in yearly_summaries.items():\n",
        "    output_file = output_dir / f\"pegasus_summary_{year}.txt\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"Mercedes F1 Infringement Summary - {year}\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "        f.write(summary)\n",
        "    print(f\"Saved {year} summary to {output_file}\")\n",
        "\n",
        "# Save overall summary\n",
        "if yearly_summaries:\n",
        "    overall_file = output_dir / \"pegasus_overall_summary.txt\"\n",
        "    with open(overall_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"Mercedes F1 Infringement Profile - Overall Summary (Pegasus)\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "        f.write(overall_summary)\n",
        "        f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\")\n",
        "        f.write(\"STATISTICS\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\")\n",
        "        total_files = sum(stats['files_processed'] for stats in yearly_stats.values())\n",
        "        total_chars = sum(stats['total_chars'] for stats in yearly_stats.values())\n",
        "        f.write(f\"Total documents processed: {total_files}\\n\")\n",
        "        f.write(f\"Total characters analyzed: {total_chars:,}\\n\")\n",
        "        f.write(f\"Overall summary length: {len(overall_summary):,} characters\\n\")\n",
        "        f.write(f\"Overall compression ratio: {(len(overall_summary)/total_chars)*100:.1f}%\\n\")\n",
        "    print(f\"Saved overall summary to {overall_file}\")\n",
        "\n",
        "print(\"\\nAll results saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
